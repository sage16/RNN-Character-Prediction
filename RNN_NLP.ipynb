{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTh-AVCFyv9y"
   },
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22280,
     "status": "ok",
     "timestamp": 1650709019510,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "rYjtBV4bhyc9",
    "outputId": "ab254448-6727-4c52-df0e-7c74022afdf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OgDZdSBy2k-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i89iiEwIZXtW"
   },
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krGDsqLMZHzN"
   },
   "outputs": [],
   "source": [
    "# Open text file and read in data as 'text'\n",
    "with open('drive/My Drive/Colab Notebooks/anna.txt', 'r') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1650651267329,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "VSqF-IjsvHS2",
    "outputId": "28562081-15e2-4131-e313-70a79ca332cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5z7HQ7vvlxl"
   },
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZJaBkNSvqBo"
   },
   "outputs": [],
   "source": [
    "#encode the text and map each character to an integer and vice versa\n",
    "\n",
    "#get each unique text\n",
    "chars = tuple(set(text))\n",
    "#maps integers to characters\n",
    "int2char = dict(enumerate(chars))\n",
    "#maps characters to unique integers\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "\n",
    "#encode the text with an array of integer\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650651267332,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "EvGyYOAK0Wwp",
    "outputId": "13ba3b47-0cad-4953-db9c-3c3561e523d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 74, 22, 17,  6, 31, 32, 21, 75, 10, 10, 10,  7, 22, 17, 17, 81,\n",
       "       21,  4, 22, 53, 36, 54, 36, 31, 18, 21, 22, 32, 31, 21, 22, 54, 54,\n",
       "       21, 22, 54, 36, 46, 31, 30, 21, 31, 67, 31, 32, 81, 21, 69, 41, 74,\n",
       "       22, 17, 17, 81, 21,  4, 22, 53, 36, 54, 81, 21, 36, 18, 21, 69, 41,\n",
       "       74, 22, 17, 17, 81, 21, 36, 41, 21, 36,  6, 18, 21, 33, 51, 41, 10,\n",
       "       51, 22, 81, 80, 10, 10, 29, 67, 31, 32, 81,  6, 74, 36, 41])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFIiHS3k0zo3"
   },
   "source": [
    "### **Pre-processing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv80E0BR09t9"
   },
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kxX-i2b0tRD"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "  #initialize the encoded array\n",
    "  one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "\n",
    "  #fill the appropriate elements with ones\n",
    "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "\n",
    "  #finally reshape it to get back the original array\n",
    "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1650651267334,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "XT6uC2dNBQnG",
    "outputId": "e10a71a5-9de0-45e2-a873-df2a096970ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#check that the function works as expected\n",
    "test_seq = np.array([[3,5,1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bE6DrILDLuP"
   },
   "source": [
    "##### Create Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWqnPR3fDQOL"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__OAPnMjKSf5"
   },
   "source": [
    "### Test Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PL1U4vXjJvXp"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1650651267341,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "8JJo7e5gKvoq",
    "outputId": "a7542ca7-ee60-456d-b729-cced8e8fc4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[23 74 22 17  6 31 32 21 75 10]\n",
      " [18 33 41 21  6 74 22  6 21 22]\n",
      " [31 41 66 21 33 32 21 22 21  4]\n",
      " [18 21  6 74 31 21  9 74 36 31]\n",
      " [21 18 22 51 21 74 31 32 21  6]\n",
      " [ 9 69 18 18 36 33 41 21 22 41]\n",
      " [21 34 41 41 22 21 74 22 66 21]\n",
      " [12 24 54 33 41 18 46 81 80 21]]\n",
      "\n",
      "y\n",
      " [[74 22 17  6 31 32 21 75 10 10]\n",
      " [33 41 21  6 74 22  6 21 22  6]\n",
      " [41 66 21 33 32 21 22 21  4 33]\n",
      " [21  6 74 31 21  9 74 36 31  4]\n",
      " [18 22 51 21 74 31 32 21  6 31]\n",
      " [69 18 18 36 33 41 21 22 41 66]\n",
      " [34 41 41 22 21 74 22 66 21 18]\n",
      " [24 54 33 41 18 46 81 80 21 78]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10,:10])\n",
    "print('\\ny\\n', y[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u75aFPvMWkX"
   },
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljf6V7uyMVqI"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "  def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                             drop_prob=0.5, lr=0.001):\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "    self.n_layers = n_layers\n",
    "    self.n_hidden = n_hidden\n",
    "    self.lr = lr\n",
    "\n",
    "    # creating character dictionaries\n",
    "    self.chars = tokens\n",
    "    self.int2char = dict(enumerate(self.chars))\n",
    "    self.char2int = {ch: ii for ii,ch in self.int2char.items()}\n",
    "\n",
    "    #define the LSTM\n",
    "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                        dropout=drop_prob, batch_first=True)\n",
    "    \n",
    "    #define a dropout layer\n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    #define the final, fully-connected output layer\n",
    "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "      ''' Forward pass throuh the network.\n",
    "      These inputs are x, and the hidden/cell state 'hidden'. '''\n",
    "\n",
    "      # get the outputs and the new hidden state from the lstm\n",
    "      r_output, hidden = self.lstm(x, hidden)\n",
    "\n",
    "      # pass through a dropout layer\n",
    "      out = self.dropout(r_output)\n",
    "\n",
    "      # Stack up LSTM outputs using view\n",
    "      out = out.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "      #put x through the fully-connected layer\n",
    "      out = self.fc(out)\n",
    "\n",
    "\n",
    "      #return the final output and the hidden state\n",
    "      return out, hidden\n",
    "\n",
    "  \n",
    "  def init_hidden(self, batch_size):\n",
    "    '''Initializes hidden stae'''\n",
    "    #create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "    #initialized to zero, for hidden state and cell state of LSTM\n",
    "    weight = next(self.parameters()).data\n",
    "    \n",
    "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY-n1_0DTweS"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvw5rlWGbJbj"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "  ''' Arguments\n",
    "      ---------\n",
    "      net: CharRNN network\n",
    "      data: text data to train the network\n",
    "      batch_size: Number of mini-sequence per mini-batch\n",
    "      seq_length: Number of character steps per mini-batch\n",
    "      lr: learning rate\n",
    "      clip: gradient clippin\n",
    "      val_frac: Fraction of data to hold out for validation\n",
    "      print every: Number of steps fpr printing training and validation loss'''\n",
    "\n",
    "  opt = optim.Adam(net.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  #create training and validation data\n",
    "  val_idx = int(len(data)*(1-val_frac))\n",
    "  data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "  counter = 0\n",
    "  n_chars = len(net.chars)\n",
    "  for e in range(epochs):\n",
    "    #initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    for x, y in get_batches(data, batch_size, seq_length):\n",
    "      counter += 1\n",
    "\n",
    "      # one-hot encode our data and make them Torch tensors\n",
    "      x = one_hot_encode(x, n_chars)\n",
    "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "      #creatin new variable for the hidden state, otherwise\n",
    "      #we'd backprop through the entire trainin history\n",
    "\n",
    "      h = tuple([each.data for each in h])\n",
    "\n",
    "      #zero accumulated gradients\n",
    "      net.zero_grad()\n",
    "\n",
    "      #get output from model\n",
    "      output, h = net(inputs, h)\n",
    "\n",
    "      #calcualte the loss and perform backprop\n",
    "      loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "      loss.backward()\n",
    "\n",
    "      #clip_grad_norm helps prevent the exploding gradient problem in RNN/LSTM\n",
    "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "      opt.step()\n",
    "\n",
    "      #loss stats\n",
    "      if counter % print_every == 0:\n",
    "        #get validation loss\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "          #one-hot encode data and make them torch tensors\n",
    "          x = one_hot_encode(x, n_chars)\n",
    "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        \n",
    "          # Creating new variables for the hidden state, otherwise\n",
    "          # we'd backprop through the entire training history\n",
    "          val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "          inputs, targets = x, y\n",
    "        \n",
    "          inputs, targets = inputs, targets\n",
    "\n",
    "          output, val_h = net(inputs, val_h)\n",
    "          val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "          val_losses.append(val_loss.item())\n",
    "                \n",
    "          net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "          print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                \"Step: {}...\".format(counter),\n",
    "                \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLt8hlMgniri"
   },
   "source": [
    "### **Instantiate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650651267672,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "vMHaxuyLghlr",
    "outputId": "a18204f7-da97-42c2-a97b-3dbb1131ceaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13615615,
     "status": "ok",
     "timestamp": 1650664883280,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "xNlQyrWSghls",
    "outputId": "ddab6155-9618-4a33-b055-f27f26791388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.3104\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.3013\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2994\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2983\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.3012\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2940\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2902\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2868\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2856\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2865\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2857\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2880\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2861\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2861\n",
      "Epoch: 1/15... Step: 10... Loss: 3.2822... Val Loss: 3.2859\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1969\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1937\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1930\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1902\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1945\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1881\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1845\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1837\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1816\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1821\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1820\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1843\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1821\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1824\n",
      "Epoch: 1/15... Step: 20... Loss: 3.1576... Val Loss: 3.1827\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1711\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1694\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1680\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1671\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1706\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1642\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1600\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1578\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1562\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1566\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1565\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1587\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1568\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1572\n",
      "Epoch: 1/15... Step: 30... Loss: 3.1469... Val Loss: 3.1569\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1573\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1569\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1568\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1563\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1601\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1528\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1491\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1470\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1450\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1456\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1453\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1479\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1462\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1465\n",
      "Epoch: 1/15... Step: 40... Loss: 3.1199... Val Loss: 3.1466\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1523\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1502\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1491\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1471\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1508\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1444\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1414\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1396\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1377\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1382\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1381\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1408\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1389\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1391\n",
      "Epoch: 1/15... Step: 50... Loss: 3.1462... Val Loss: 3.1391\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1466\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1461\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1440\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1422\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1465\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1397\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1362\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1343\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1328\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1336\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1333\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1357\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1341\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1345\n",
      "Epoch: 1/15... Step: 60... Loss: 3.1233... Val Loss: 3.1343\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1420\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1442\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1435\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1421\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1465\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1389\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1349\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1327\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1307\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1311\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1309\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1331\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1311\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1315\n",
      "Epoch: 1/15... Step: 70... Loss: 3.1107... Val Loss: 3.1314\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1374\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1385\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1379\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1367\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1403\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1332\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1293\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1271\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1254\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1257\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1254\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1279\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1261\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1264\n",
      "Epoch: 1/15... Step: 80... Loss: 3.1240... Val Loss: 3.1266\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1322\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1294\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1282\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1270\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1308\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1241\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1206\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1187\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1166\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1171\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1171\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1196\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1177\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1182\n",
      "Epoch: 1/15... Step: 90... Loss: 3.1226... Val Loss: 3.1183\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1149\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1134\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1130\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1118\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1159\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1088\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1056\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1035\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1019\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1025\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1021\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1044\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1026\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1030\n",
      "Epoch: 1/15... Step: 100... Loss: 3.0991... Val Loss: 3.1029\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0993\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0981\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0977\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0961\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.1013\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0935\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0897\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0874\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0853\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0864\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0861\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0887\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0870\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0873\n",
      "Epoch: 1/15... Step: 110... Loss: 3.0912... Val Loss: 3.0873\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0260\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0238\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0222\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0198\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0228\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0162\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0121\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0093\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0069\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0075\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0073\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0100\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0084\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0087\n",
      "Epoch: 1/15... Step: 120... Loss: 2.9995... Val Loss: 3.0090\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9221\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9167\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9119\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9085\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9119\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9046\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.9013\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8981\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8952\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8959\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8957\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8984\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8965\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8965\n",
      "Epoch: 1/15... Step: 130... Loss: 2.9251... Val Loss: 2.8972\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7971\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7896\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7859\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7824\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7851\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7771\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7735\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7700\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7668\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7678\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7676\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7705\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7682\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7679\n",
      "Epoch: 2/15... Step: 140... Loss: 2.7914... Val Loss: 2.7692\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7513\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7396\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7326\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7292\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7324\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7253\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7222\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7193\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7156\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7171\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7174\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7198\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7181\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7179\n",
      "Epoch: 2/15... Step: 150... Loss: 2.7290... Val Loss: 2.7191\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6614\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6533\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6456\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6405\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6428\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6338\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6304\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6275\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6239\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6253\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6253\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6279\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6259\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6252\n",
      "Epoch: 2/15... Step: 160... Loss: 2.6234... Val Loss: 2.6264\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.6105\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5982\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5923\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5886\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5901\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5791\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5751\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5717\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5679\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5696\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5696\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5725\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5709\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5705\n",
      "Epoch: 2/15... Step: 170... Loss: 2.5431... Val Loss: 2.5723\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5735\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5602\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5525\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5488\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5504\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5392\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5352\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5319\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5284\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5298\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5293\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5323\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5307\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5307\n",
      "Epoch: 2/15... Step: 180... Loss: 2.5081... Val Loss: 2.5327\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.5389\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.5222\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.5160\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.5120\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.5131\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.5033\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4985\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4948\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4914\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4925\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4922\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4948\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4929\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4924\n",
      "Epoch: 2/15... Step: 190... Loss: 2.4579... Val Loss: 2.4945\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.5185\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.5087\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.5042\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.5009\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.5029\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4905\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4847\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4813\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4776\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4785\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4783\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4814\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4797\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4798\n",
      "Epoch: 2/15... Step: 200... Loss: 2.4531... Val Loss: 2.4820\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4895\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4717\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4642\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4611\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4634\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4523\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4466\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4432\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4383\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4392\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4387\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4420\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4404\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4400\n",
      "Epoch: 2/15... Step: 210... Loss: 2.4106... Val Loss: 2.4425\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4611\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4441\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4383\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4349\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4374\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4242\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4191\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4152\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4107\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4121\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4117\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4149\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4133\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4131\n",
      "Epoch: 2/15... Step: 220... Loss: 2.3817... Val Loss: 2.4156\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.4317\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.4192\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.4140\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.4105\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.4132\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.4008\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3961\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3918\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3874\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3889\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3882\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3912\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3894\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3890\n",
      "Epoch: 2/15... Step: 230... Loss: 2.3680... Val Loss: 2.3915\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.4102\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3951\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3890\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3882\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3915\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3800\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3737\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3695\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3648\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3662\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3648\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3674\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3656\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3654\n",
      "Epoch: 2/15... Step: 240... Loss: 2.3464... Val Loss: 2.3678\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3862\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3720\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3659\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3644\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3675\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3552\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3487\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3446\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3399\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3407\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3401\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3434\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3417\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3412\n",
      "Epoch: 2/15... Step: 250... Loss: 2.2851... Val Loss: 2.3439\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3687\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3525\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3440\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3412\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3446\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3317\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3258\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3211\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3158\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3168\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3163\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3189\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3170\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3165\n",
      "Epoch: 2/15... Step: 260... Loss: 2.2663... Val Loss: 2.3192\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3340\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3279\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3192\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3177\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3221\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3092\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.3040\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2993\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2945\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2959\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2953\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2982\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2963\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2962\n",
      "Epoch: 2/15... Step: 270... Loss: 2.2685... Val Loss: 2.2987\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.3201\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.3066\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2966\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2973\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.3019\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2885\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2825\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2782\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2738\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2750\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2745\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2771\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2758\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2748\n",
      "Epoch: 3/15... Step: 280... Loss: 2.2557... Val Loss: 2.2775\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2976\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2826\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2739\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2733\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2789\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2664\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2607\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2568\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2518\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2537\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2536\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2564\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2545\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2542\n",
      "Epoch: 3/15... Step: 290... Loss: 2.2247... Val Loss: 2.2567\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2724\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2630\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2531\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2526\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2597\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2478\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2415\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2379\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2329\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2342\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2341\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2369\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2349\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2342\n",
      "Epoch: 3/15... Step: 300... Loss: 2.2033... Val Loss: 2.2363\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2554\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2445\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2341\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2336\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2390\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2270\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2209\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2165\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2108\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2127\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2124\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2150\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2130\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2129\n",
      "Epoch: 3/15... Step: 310... Loss: 2.1622... Val Loss: 2.2152\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2477\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2307\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2192\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2178\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2229\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2090\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.2034\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1984\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1935\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1954\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1953\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1980\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1957\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1947\n",
      "Epoch: 3/15... Step: 320... Loss: 2.1444... Val Loss: 2.1973\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.2161\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.2058\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1956\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1950\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.2001\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1871\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1820\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1773\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1715\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1736\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1737\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1770\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1750\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1742\n",
      "Epoch: 3/15... Step: 330... Loss: 2.1025... Val Loss: 2.1766\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1999\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1860\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1735\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1728\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1786\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1657\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1610\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1564\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1503\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1525\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1524\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1560\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1549\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1546\n",
      "Epoch: 3/15... Step: 340... Loss: 2.1244... Val Loss: 2.1572\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1863\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1777\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1670\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1666\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1724\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1592\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1540\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1490\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1433\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1452\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1462\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1491\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1473\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1465\n",
      "Epoch: 3/15... Step: 350... Loss: 2.1046... Val Loss: 2.1489\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1704\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1568\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1445\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1431\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1493\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1359\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1309\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1265\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1208\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1238\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1238\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1266\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1240\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1236\n",
      "Epoch: 3/15... Step: 360... Loss: 2.0339... Val Loss: 2.1261\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1561\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1453\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1360\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1367\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1422\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1286\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1219\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1177\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1109\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1132\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1133\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1162\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1141\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1135\n",
      "Epoch: 3/15... Step: 370... Loss: 2.0622... Val Loss: 2.1162\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.1370\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.1267\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.1127\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.1117\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.1161\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.1035\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0977\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0940\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0886\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0905\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0909\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0935\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0919\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0913\n",
      "Epoch: 3/15... Step: 380... Loss: 2.0316... Val Loss: 2.0933\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.1275\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.1152\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.1014\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.1005\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.1049\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0914\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0861\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0813\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0750\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0769\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0776\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0804\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0787\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0777\n",
      "Epoch: 3/15... Step: 390... Loss: 2.0085... Val Loss: 2.0806\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.1109\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0971\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0847\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0834\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0900\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0781\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0718\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0666\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0601\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0622\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0635\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0664\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0648\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0642\n",
      "Epoch: 3/15... Step: 400... Loss: 1.9831... Val Loss: 2.0671\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0894\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0811\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0691\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0680\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0760\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0621\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0570\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0526\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0466\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0479\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0489\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0516\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0503\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0496\n",
      "Epoch: 3/15... Step: 410... Loss: 1.9922... Val Loss: 2.0524\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0780\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0705\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0536\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0519\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0591\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0457\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0401\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0363\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0304\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0319\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0327\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0357\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0341\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0335\n",
      "Epoch: 4/15... Step: 420... Loss: 1.9763... Val Loss: 2.0362\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0598\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0499\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0354\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0340\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0411\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0289\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0248\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0212\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0152\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0167\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0178\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0202\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0186\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0175\n",
      "Epoch: 4/15... Step: 430... Loss: 1.9683... Val Loss: 2.0203\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0543\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0433\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0261\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0234\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0295\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0176\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0127\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0082\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0019\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0038\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0046\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0074\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0060\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0050\n",
      "Epoch: 4/15... Step: 440... Loss: 1.9509... Val Loss: 2.0081\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0470\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0313\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0115\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0129\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0199\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0077\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 2.0019\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9979\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9922\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9941\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9947\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9978\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9962\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9957\n",
      "Epoch: 4/15... Step: 450... Loss: 1.8931... Val Loss: 1.9981\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 2.0430\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 2.0271\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 2.0090\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 2.0077\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 2.0124\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9988\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9930\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9882\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9816\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9837\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9847\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9867\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9854\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9844\n",
      "Epoch: 4/15... Step: 460... Loss: 1.8766... Val Loss: 1.9870\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 2.0248\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 2.0164\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9975\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9964\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 2.0018\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9881\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9840\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9790\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9729\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9747\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9750\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9776\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9756\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9744\n",
      "Epoch: 4/15... Step: 470... Loss: 1.9074... Val Loss: 1.9769\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 2.0114\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9969\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9774\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9760\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9816\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9699\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9639\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9604\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9553\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9572\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9581\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9610\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9594\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9590\n",
      "Epoch: 4/15... Step: 480... Loss: 1.8851... Val Loss: 1.9623\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 2.0174\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9973\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9757\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9748\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9815\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9668\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9613\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9571\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9506\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9523\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9533\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9557\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9539\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9528\n",
      "Epoch: 4/15... Step: 490... Loss: 1.8939... Val Loss: 1.9561\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9856\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9728\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9531\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9540\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9611\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9489\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9444\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9396\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9331\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9349\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9366\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9393\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9375\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9363\n",
      "Epoch: 4/15... Step: 500... Loss: 1.8873... Val Loss: 1.9397\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9915\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9731\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9509\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9511\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9559\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9426\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9371\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9322\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9253\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9259\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9264\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9295\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9280\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9271\n",
      "Epoch: 4/15... Step: 510... Loss: 1.8553... Val Loss: 1.9302\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9739\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9590\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9353\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9363\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9425\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9305\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9261\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9219\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9161\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9171\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9190\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9220\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9210\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9203\n",
      "Epoch: 4/15... Step: 520... Loss: 1.8672... Val Loss: 1.9233\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9596\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9499\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9267\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9270\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9324\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9192\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9137\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9096\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9044\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9057\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9067\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9093\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9076\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9071\n",
      "Epoch: 4/15... Step: 530... Loss: 1.8294... Val Loss: 1.9103\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9585\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9423\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9197\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9185\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9251\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9125\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9058\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.8997\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.8933\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.8954\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.8976\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9009\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.8994\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.8988\n",
      "Epoch: 4/15... Step: 540... Loss: 1.7936... Val Loss: 1.9018\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.9433\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.9315\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.9085\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.9081\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.9155\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.9022\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8971\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8920\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8858\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8866\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8880\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8910\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8894\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8887\n",
      "Epoch: 4/15... Step: 550... Loss: 1.8338... Val Loss: 1.8914\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.9367\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.9226\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8993\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8977\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.9045\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8908\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8851\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8816\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8751\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8767\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8780\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8808\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8801\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8796\n",
      "Epoch: 5/15... Step: 560... Loss: 1.7955... Val Loss: 1.8826\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.9272\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.9131\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8891\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8889\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8948\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8820\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8773\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8715\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8652\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8666\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8667\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8698\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8690\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8677\n",
      "Epoch: 5/15... Step: 570... Loss: 1.7764... Val Loss: 1.8709\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.9190\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.9006\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8742\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8736\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8807\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8696\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8650\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8614\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8558\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8569\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8583\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8616\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8606\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8601\n",
      "Epoch: 5/15... Step: 580... Loss: 1.7640... Val Loss: 1.8630\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.9157\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8955\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8727\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8682\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8727\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8608\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8568\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8521\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8460\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8483\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8501\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8533\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8526\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8522\n",
      "Epoch: 5/15... Step: 590... Loss: 1.7673... Val Loss: 1.8555\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.9082\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8896\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8639\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8632\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8685\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8568\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8528\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8487\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8418\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8435\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8452\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8477\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8466\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8461\n",
      "Epoch: 5/15... Step: 600... Loss: 1.7531... Val Loss: 1.8490\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8893\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8813\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8577\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8557\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8615\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8484\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8449\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8406\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8344\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8356\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8369\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8396\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8385\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8383\n",
      "Epoch: 5/15... Step: 610... Loss: 1.7350... Val Loss: 1.8411\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8770\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8685\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8437\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8461\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8537\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8416\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8363\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8309\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8251\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8270\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8289\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8325\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8320\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8315\n",
      "Epoch: 5/15... Step: 620... Loss: 1.7376... Val Loss: 1.8343\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8836\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8715\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8456\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8420\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8489\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8356\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8314\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8261\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8201\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8210\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8225\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8252\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8244\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8232\n",
      "Epoch: 5/15... Step: 630... Loss: 1.7567... Val Loss: 1.8263\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8799\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8637\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8360\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8349\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8405\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8275\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8229\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8193\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8136\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8150\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8159\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8189\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8178\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8169\n",
      "Epoch: 5/15... Step: 640... Loss: 1.7181... Val Loss: 1.8199\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8725\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8585\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8302\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8269\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8345\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8219\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8164\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8114\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8052\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8062\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8073\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8104\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8089\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8078\n",
      "Epoch: 5/15... Step: 650... Loss: 1.7034... Val Loss: 1.8106\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8634\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8493\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8248\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8235\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8301\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8177\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8126\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8079\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8016\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8024\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8031\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8064\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8054\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8041\n",
      "Epoch: 5/15... Step: 660... Loss: 1.6762... Val Loss: 1.8073\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8563\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8404\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8109\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8097\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8173\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8046\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.8008\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7964\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7908\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7926\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7933\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7960\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7955\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7951\n",
      "Epoch: 5/15... Step: 670... Loss: 1.7121... Val Loss: 1.7982\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.8523\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.8366\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.8077\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.8059\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.8146\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.8013\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7960\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7909\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7843\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7853\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7871\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7904\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7894\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7888\n",
      "Epoch: 5/15... Step: 680... Loss: 1.7070... Val Loss: 1.7919\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.8375\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.8283\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.8018\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7998\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.8066\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7944\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7885\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7833\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7768\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7779\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7790\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7824\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7817\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7806\n",
      "Epoch: 5/15... Step: 690... Loss: 1.6790... Val Loss: 1.7832\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.8447\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.8269\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7980\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7961\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.8012\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7888\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7833\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7795\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7718\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7727\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7739\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7764\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7758\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7749\n",
      "Epoch: 6/15... Step: 700... Loss: 1.6828... Val Loss: 1.7783\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.8340\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.8222\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7951\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7922\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7980\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7839\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7785\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7721\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7660\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7674\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7688\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7724\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7715\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7710\n",
      "Epoch: 6/15... Step: 710... Loss: 1.6714... Val Loss: 1.7736\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.8226\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.8101\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7837\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7821\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7871\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7742\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7696\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7650\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7587\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7595\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7602\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7639\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7627\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7619\n",
      "Epoch: 6/15... Step: 720... Loss: 1.6563... Val Loss: 1.7650\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.8226\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.8066\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7777\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7747\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7803\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7665\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7626\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7578\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7514\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7531\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7542\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7576\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7567\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7559\n",
      "Epoch: 6/15... Step: 730... Loss: 1.6702... Val Loss: 1.7588\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.8155\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.8022\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7687\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7659\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7722\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7618\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7574\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7530\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7464\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7478\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7492\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7515\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7505\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7501\n",
      "Epoch: 6/15... Step: 740... Loss: 1.6353... Val Loss: 1.7533\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.8223\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.8039\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7717\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7685\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7751\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7629\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7567\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7516\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7445\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7454\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7471\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7499\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7497\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7489\n",
      "Epoch: 6/15... Step: 750... Loss: 1.6164... Val Loss: 1.7521\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.8027\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7897\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7599\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7581\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7651\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7535\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7488\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7435\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7373\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7391\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7408\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7445\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7431\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7427\n",
      "Epoch: 6/15... Step: 760... Loss: 1.6507... Val Loss: 1.7459\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7997\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7879\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7579\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7571\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7658\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7539\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7488\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7430\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7372\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7380\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7387\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7418\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7411\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7407\n",
      "Epoch: 6/15... Step: 770... Loss: 1.6350... Val Loss: 1.7439\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.8000\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7805\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7512\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7490\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7555\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7418\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7364\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7312\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7253\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7269\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7287\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7332\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7326\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7313\n",
      "Epoch: 6/15... Step: 780... Loss: 1.6220... Val Loss: 1.7344\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7929\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7833\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7536\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7491\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7554\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7418\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7361\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7307\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7242\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7253\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7258\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7291\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7282\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7272\n",
      "Epoch: 6/15... Step: 790... Loss: 1.6115... Val Loss: 1.7303\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7901\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7780\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7477\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7464\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7525\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7384\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7324\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7281\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7211\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7218\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7229\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7260\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7250\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7242\n",
      "Epoch: 6/15... Step: 800... Loss: 1.6255... Val Loss: 1.7271\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7846\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7699\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7394\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7377\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7449\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7324\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7275\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7224\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7157\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7175\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7186\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7221\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7210\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7203\n",
      "Epoch: 6/15... Step: 810... Loss: 1.6076... Val Loss: 1.7238\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7779\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7620\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7313\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7304\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7376\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7243\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7190\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7136\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7073\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7083\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7095\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7128\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7118\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7112\n",
      "Epoch: 6/15... Step: 820... Loss: 1.5705... Val Loss: 1.7142\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7717\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7567\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7258\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7248\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7325\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7191\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7134\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7088\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7026\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7038\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7048\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7079\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7073\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7072\n",
      "Epoch: 6/15... Step: 830... Loss: 1.6114... Val Loss: 1.7101\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7637\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7534\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7214\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7189\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7241\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7129\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7080\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7031\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.6971\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.6975\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.6986\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7018\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7016\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7012\n",
      "Epoch: 7/15... Step: 840... Loss: 1.5777... Val Loss: 1.7040\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7659\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7514\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7209\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7159\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7230\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7119\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7065\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7003\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.6933\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.6948\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.6961\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7003\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.6999\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.6994\n",
      "Epoch: 7/15... Step: 850... Loss: 1.5897... Val Loss: 1.7029\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7580\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7436\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7127\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7084\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7171\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7048\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.7006\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6944\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6874\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6890\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6907\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6939\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6935\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6929\n",
      "Epoch: 7/15... Step: 860... Loss: 1.5662... Val Loss: 1.6963\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.7470\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.7381\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.7072\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.7064\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.7127\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.7006\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6958\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6906\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6843\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6859\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6870\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6896\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6895\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6891\n",
      "Epoch: 7/15... Step: 870... Loss: 1.5751... Val Loss: 1.6921\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.7488\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.7375\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.7049\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.7029\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.7101\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6982\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6936\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6880\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6822\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6840\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6849\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6882\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6871\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6865\n",
      "Epoch: 7/15... Step: 880... Loss: 1.5836... Val Loss: 1.6897\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.7478\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.7364\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.7044\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.7015\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.7063\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6937\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6874\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6818\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6750\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6763\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6777\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6811\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6808\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6798\n",
      "Epoch: 7/15... Step: 890... Loss: 1.5734... Val Loss: 1.6823\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.7421\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.7296\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6957\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6944\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.7011\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6884\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6827\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6771\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6709\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6721\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6729\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6762\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6750\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6742\n",
      "Epoch: 7/15... Step: 900... Loss: 1.5658... Val Loss: 1.6772\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.7435\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.7257\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6925\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6909\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6979\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6860\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6802\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6738\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6674\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6683\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6696\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6730\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6723\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6718\n",
      "Epoch: 7/15... Step: 910... Loss: 1.5264... Val Loss: 1.6754\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.7432\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.7287\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6925\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6893\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6955\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6834\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6771\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6718\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6662\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6677\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6682\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6722\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6721\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6709\n",
      "Epoch: 7/15... Step: 920... Loss: 1.5547... Val Loss: 1.6739\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.7357\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.7221\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6881\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6843\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6923\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6802\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6734\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6687\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6631\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6640\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6650\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6688\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6682\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6676\n",
      "Epoch: 7/15... Step: 930... Loss: 1.5384... Val Loss: 1.6705\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.7281\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.7135\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6801\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6801\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6860\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6735\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6678\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6627\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6562\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6572\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6585\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6622\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6621\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6615\n",
      "Epoch: 7/15... Step: 940... Loss: 1.5388... Val Loss: 1.6642\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.7227\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.7039\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6716\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6701\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6771\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6653\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6593\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6533\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6480\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6492\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6503\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6540\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6535\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6537\n",
      "Epoch: 7/15... Step: 950... Loss: 1.5572... Val Loss: 1.6570\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.7107\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6978\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6652\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6622\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6703\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6596\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6545\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6494\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6425\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6446\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6463\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6499\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6496\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6494\n",
      "Epoch: 7/15... Step: 960... Loss: 1.5444... Val Loss: 1.6523\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.7185\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.7008\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6659\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6639\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6715\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6598\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6547\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6507\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6441\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6453\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6459\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6492\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6490\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6479\n",
      "Epoch: 7/15... Step: 970... Loss: 1.5563... Val Loss: 1.6513\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.7218\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.7047\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6705\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6669\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6718\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6598\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6544\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6485\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6423\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6427\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6437\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6471\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6472\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6468\n",
      "Epoch: 8/15... Step: 980... Loss: 1.5291... Val Loss: 1.6498\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.7143\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.7003\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6661\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6661\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6740\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6616\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6568\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6501\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6434\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6431\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6432\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6465\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6463\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6457\n",
      "Epoch: 8/15... Step: 990... Loss: 1.5280... Val Loss: 1.6491\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.7085\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6930\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6588\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6565\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6643\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6527\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6470\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6411\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6345\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6365\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6379\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6414\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6405\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6403\n",
      "Epoch: 8/15... Step: 1000... Loss: 1.5158... Val Loss: 1.6435\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.7042\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6835\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6515\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6504\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6576\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6444\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6402\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6343\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6272\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6291\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6299\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6341\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6344\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6336\n",
      "Epoch: 8/15... Step: 1010... Loss: 1.5523... Val Loss: 1.6367\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.7092\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6913\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6573\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6548\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6592\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6458\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6414\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6363\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6296\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6305\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6307\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6344\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6342\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6335\n",
      "Epoch: 8/15... Step: 1020... Loss: 1.5268... Val Loss: 1.6361\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.7003\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6822\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6490\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6481\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6562\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6423\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6368\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6307\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6238\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6249\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6263\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6300\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6294\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6292\n",
      "Epoch: 8/15... Step: 1030... Loss: 1.5073... Val Loss: 1.6314\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6926\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6854\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6511\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6488\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6553\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6418\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6345\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6278\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6210\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6223\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6239\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6280\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6279\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6273\n",
      "Epoch: 8/15... Step: 1040... Loss: 1.5172... Val Loss: 1.6308\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6878\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6768\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6425\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6435\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6503\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6383\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6319\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6272\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6203\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6222\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6227\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6258\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6253\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6247\n",
      "Epoch: 8/15... Step: 1050... Loss: 1.4851... Val Loss: 1.6277\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6880\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6695\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6347\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6334\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6409\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6300\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6237\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6180\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6117\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6138\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6147\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6185\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6181\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6184\n",
      "Epoch: 8/15... Step: 1060... Loss: 1.4983... Val Loss: 1.6215\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6772\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6666\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6342\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6336\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6402\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6276\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6214\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6166\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6103\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6117\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6131\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6170\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6166\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6161\n",
      "Epoch: 8/15... Step: 1070... Loss: 1.5020... Val Loss: 1.6195\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6848\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6675\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6359\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6347\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6424\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6309\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6256\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6190\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6122\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6130\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6133\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6173\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6165\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6163\n",
      "Epoch: 8/15... Step: 1080... Loss: 1.5008... Val Loss: 1.6190\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6774\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6612\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6270\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6266\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6324\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6209\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6147\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6095\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6031\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6042\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6047\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6077\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6075\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6074\n",
      "Epoch: 8/15... Step: 1090... Loss: 1.4834... Val Loss: 1.6104\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6776\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6604\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6269\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6253\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6321\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6205\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6143\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6090\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6026\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6030\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6038\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6075\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6071\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6067\n",
      "Epoch: 8/15... Step: 1100... Loss: 1.4680... Val Loss: 1.6097\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6739\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6558\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6238\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6186\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6255\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6146\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6093\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6043\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.5984\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.5990\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6005\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6043\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6034\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6029\n",
      "Epoch: 8/15... Step: 1110... Loss: 1.4767... Val Loss: 1.6062\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6690\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6566\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6200\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6190\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6263\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6147\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6095\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6041\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.5973\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.5988\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.5994\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6032\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6030\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6019\n",
      "Epoch: 9/15... Step: 1120... Loss: 1.4958... Val Loss: 1.6052\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6608\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6479\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6176\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6171\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6230\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6124\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6061\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6007\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.5948\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.5969\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.5979\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6016\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6014\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6006\n",
      "Epoch: 9/15... Step: 1130... Loss: 1.4898... Val Loss: 1.6034\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6691\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6472\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6131\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6126\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6204\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6096\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.6028\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5978\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5914\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5926\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5935\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5974\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5967\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5965\n",
      "Epoch: 9/15... Step: 1140... Loss: 1.4883... Val Loss: 1.5995\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6655\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6477\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6131\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6114\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6195\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6060\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.6015\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5960\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5892\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5893\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5907\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5941\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5937\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5928\n",
      "Epoch: 9/15... Step: 1150... Loss: 1.5055... Val Loss: 1.5960\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.6568\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.6417\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.6129\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.6105\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.6157\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.6041\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5983\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5934\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5865\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5890\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5902\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5937\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5934\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5929\n",
      "Epoch: 9/15... Step: 1160... Loss: 1.4484... Val Loss: 1.5962\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.6615\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.6415\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.6076\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.6046\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.6140\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.6030\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5975\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5921\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5860\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5872\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5880\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5921\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5920\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5915\n",
      "Epoch: 9/15... Step: 1170... Loss: 1.4710... Val Loss: 1.5948\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.6529\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.6402\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.6092\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.6058\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.6125\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.6005\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5949\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5893\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5830\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5844\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5861\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5897\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5892\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5887\n",
      "Epoch: 9/15... Step: 1180... Loss: 1.4542... Val Loss: 1.5917\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.6607\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.6448\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.6101\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.6076\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.6157\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.6031\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5970\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5905\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5832\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5840\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5845\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5883\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5880\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5876\n",
      "Epoch: 9/15... Step: 1190... Loss: 1.4896... Val Loss: 1.5908\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.6480\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.6343\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.6010\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5977\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.6043\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5925\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5863\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5798\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5743\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5754\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5770\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5805\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5797\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5790\n",
      "Epoch: 9/15... Step: 1200... Loss: 1.4352... Val Loss: 1.5822\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.6454\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.6303\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5961\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5956\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.6033\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5921\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5866\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5798\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5733\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5742\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5750\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5787\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5782\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5782\n",
      "Epoch: 9/15... Step: 1210... Loss: 1.4525... Val Loss: 1.5813\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.6468\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.6286\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5959\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5924\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5998\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5888\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5837\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5773\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5703\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5717\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5734\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5765\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5765\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5761\n",
      "Epoch: 9/15... Step: 1220... Loss: 1.4459... Val Loss: 1.5791\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.6379\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.6211\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5875\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5865\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5937\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5832\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5776\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5722\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5655\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5671\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5687\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5729\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5732\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5726\n",
      "Epoch: 9/15... Step: 1230... Loss: 1.4342... Val Loss: 1.5762\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.6429\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.6265\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5923\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5916\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5973\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5869\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5821\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5752\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5684\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5695\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5712\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5753\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5749\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5748\n",
      "Epoch: 9/15... Step: 1240... Loss: 1.4441... Val Loss: 1.5777\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.6412\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.6195\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5864\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5842\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5928\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5818\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5754\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5704\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5641\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5655\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5660\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5702\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5697\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5687\n",
      "Epoch: 9/15... Step: 1250... Loss: 1.4495... Val Loss: 1.5718\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.6437\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.6276\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5925\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5893\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5961\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5849\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5784\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5728\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5657\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5655\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5655\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5691\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5689\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5680\n",
      "Epoch: 10/15... Step: 1260... Loss: 1.4471... Val Loss: 1.5714\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.6281\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.6161\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5839\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5814\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5881\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5767\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5715\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5667\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5604\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5623\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5636\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5669\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5666\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5654\n",
      "Epoch: 10/15... Step: 1270... Loss: 1.4445... Val Loss: 1.5690\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.6283\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.6134\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5811\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5783\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5851\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5733\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5680\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5625\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5567\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5583\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5583\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5627\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5625\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5618\n",
      "Epoch: 10/15... Step: 1280... Loss: 1.4522... Val Loss: 1.5647\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.6357\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.6184\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5826\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5807\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5861\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5753\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5694\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5626\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5564\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5575\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5584\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5624\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5625\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5613\n",
      "Epoch: 10/15... Step: 1290... Loss: 1.4359... Val Loss: 1.5646\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.6333\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.6152\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5831\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5822\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5884\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5762\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5697\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5628\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5557\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5574\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5578\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5619\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5620\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5614\n",
      "Epoch: 10/15... Step: 1300... Loss: 1.4277... Val Loss: 1.5651\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.6239\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.6076\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5747\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5722\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5797\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5690\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5621\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5564\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5502\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5515\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5531\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5567\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5567\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5562\n",
      "Epoch: 10/15... Step: 1310... Loss: 1.4481... Val Loss: 1.5596\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.6261\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.6095\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5759\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5724\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5794\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5684\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5639\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5581\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5515\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5522\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5534\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5571\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5574\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5568\n",
      "Epoch: 10/15... Step: 1320... Loss: 1.4061... Val Loss: 1.5598\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.6280\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.6140\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5785\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5763\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5821\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5687\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5636\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5570\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5496\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5514\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5529\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5573\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5566\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5560\n",
      "Epoch: 10/15... Step: 1330... Loss: 1.4147... Val Loss: 1.5591\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.6184\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.6040\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5678\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5651\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5711\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5594\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5532\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5470\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5411\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5426\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5442\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5485\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5483\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5474\n",
      "Epoch: 10/15... Step: 1340... Loss: 1.4107... Val Loss: 1.5505\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.6195\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.6009\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5641\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5620\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5682\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5581\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5531\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5477\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5419\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5427\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5443\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5483\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5478\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5474\n",
      "Epoch: 10/15... Step: 1350... Loss: 1.4042... Val Loss: 1.5506\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.6128\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5956\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5613\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5599\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5670\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5568\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5518\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5459\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5399\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5402\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5415\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5457\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5453\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5445\n",
      "Epoch: 10/15... Step: 1360... Loss: 1.3984... Val Loss: 1.5480\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.6125\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5936\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5598\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5586\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5647\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5537\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5479\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5419\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5369\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5381\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5399\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5440\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5435\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5432\n",
      "Epoch: 10/15... Step: 1370... Loss: 1.3922... Val Loss: 1.5469\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.6160\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5939\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5588\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5585\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5671\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5555\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5477\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5425\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5357\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5367\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5379\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5415\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5415\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5413\n",
      "Epoch: 10/15... Step: 1380... Loss: 1.4324... Val Loss: 1.5443\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.6058\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5921\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5540\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5534\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5593\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5482\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5426\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5379\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5320\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5325\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5336\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5369\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5367\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5358\n",
      "Epoch: 10/15... Step: 1390... Loss: 1.4373... Val Loss: 1.5395\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.6095\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5931\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5549\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5526\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5615\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5502\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5434\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5382\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5326\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5332\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5341\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5380\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5379\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5377\n",
      "Epoch: 11/15... Step: 1400... Loss: 1.4429... Val Loss: 1.5404\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.6029\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5856\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5510\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5506\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5577\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5466\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5395\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5324\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5261\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5278\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5298\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5333\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5330\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5321\n",
      "Epoch: 11/15... Step: 1410... Loss: 1.4488... Val Loss: 1.5354\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.6031\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5870\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5536\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5503\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5578\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5467\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5417\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5359\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5290\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5308\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5318\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5351\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5354\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5344\n",
      "Epoch: 11/15... Step: 1420... Loss: 1.4334... Val Loss: 1.5376\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.6109\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5895\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5547\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5516\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5576\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5465\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5408\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5354\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5284\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5296\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5307\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5351\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5352\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5340\n",
      "Epoch: 11/15... Step: 1430... Loss: 1.3995... Val Loss: 1.5373\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.6064\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5910\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5565\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5511\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5576\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5461\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5403\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5343\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5278\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5279\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5288\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5326\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5323\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5317\n",
      "Epoch: 11/15... Step: 1440... Loss: 1.4251... Val Loss: 1.5349\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.6075\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5862\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5535\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5485\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5550\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5439\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5378\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5319\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5251\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5273\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5286\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5318\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5310\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5300\n",
      "Epoch: 11/15... Step: 1450... Loss: 1.3657... Val Loss: 1.5332\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5935\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5800\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5453\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5450\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5527\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5421\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5380\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5314\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5258\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5270\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5273\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5315\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5309\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5303\n",
      "Epoch: 11/15... Step: 1460... Loss: 1.3828... Val Loss: 1.5337\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.6021\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5859\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5522\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5484\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5551\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5440\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5375\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5301\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5242\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5248\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5255\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5296\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5294\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5291\n",
      "Epoch: 11/15... Step: 1470... Loss: 1.3777... Val Loss: 1.5326\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5995\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5815\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5465\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5417\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5484\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5369\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5311\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5251\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5196\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5202\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5219\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5260\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5251\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5242\n",
      "Epoch: 11/15... Step: 1480... Loss: 1.3982... Val Loss: 1.5269\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5881\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5758\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5397\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5388\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5456\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5346\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5290\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5238\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5169\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5183\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5195\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5227\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5230\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5225\n",
      "Epoch: 11/15... Step: 1490... Loss: 1.3852... Val Loss: 1.5256\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5980\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5798\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5442\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5421\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5496\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5379\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5329\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5263\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5191\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5196\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5205\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5246\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5239\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5232\n",
      "Epoch: 11/15... Step: 1500... Loss: 1.3706... Val Loss: 1.5267\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5867\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5668\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5296\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5270\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5355\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5250\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5206\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5166\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5113\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5119\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5130\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5173\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5175\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5170\n",
      "Epoch: 11/15... Step: 1510... Loss: 1.3546... Val Loss: 1.5200\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5861\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5623\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5280\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5255\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5337\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5248\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5201\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5139\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5084\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5086\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5097\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5137\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5138\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5134\n",
      "Epoch: 11/15... Step: 1520... Loss: 1.3967... Val Loss: 1.5167\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5854\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5663\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5308\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5307\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5374\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5269\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5209\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5160\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5104\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5104\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5112\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5147\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5149\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5147\n",
      "Epoch: 12/15... Step: 1530... Loss: 1.4340... Val Loss: 1.5180\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5878\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5685\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5328\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5301\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5372\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5273\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5206\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5138\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5084\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5088\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5107\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5144\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5145\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5134\n",
      "Epoch: 12/15... Step: 1540... Loss: 1.3987... Val Loss: 1.5168\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5781\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5633\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5290\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5267\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5335\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5246\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5183\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5126\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5067\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5077\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5087\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5122\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5121\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5115\n",
      "Epoch: 12/15... Step: 1550... Loss: 1.4032... Val Loss: 1.5148\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5908\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5683\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5334\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5288\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5332\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5216\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5151\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5094\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5036\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5050\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5068\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5114\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5117\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5110\n",
      "Epoch: 12/15... Step: 1560... Loss: 1.4009... Val Loss: 1.5139\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5821\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5689\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5342\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5299\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5357\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5238\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5191\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5129\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5065\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5071\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5083\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5118\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5117\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5110\n",
      "Epoch: 12/15... Step: 1570... Loss: 1.3523... Val Loss: 1.5148\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5782\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5604\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5241\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5237\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5318\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5208\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5147\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5082\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5019\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5036\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5048\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5085\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5089\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5080\n",
      "Epoch: 12/15... Step: 1580... Loss: 1.3364... Val Loss: 1.5115\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5857\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5640\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5260\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5235\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5317\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5208\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5154\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5087\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5026\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5038\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5050\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5093\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5095\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5085\n",
      "Epoch: 12/15... Step: 1590... Loss: 1.3337... Val Loss: 1.5124\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5758\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5582\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5242\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5216\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5280\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5170\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5116\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5057\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.4992\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5006\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5022\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5059\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5058\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5053\n",
      "Epoch: 12/15... Step: 1600... Loss: 1.3594... Val Loss: 1.5087\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5830\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5609\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5275\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5275\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5354\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5216\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5159\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5096\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5033\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5044\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5054\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5090\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5089\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5081\n",
      "Epoch: 12/15... Step: 1610... Loss: 1.3502... Val Loss: 1.5113\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5816\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5578\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5243\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5236\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5298\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5185\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5128\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5067\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5011\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5020\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5028\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5054\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5046\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5041\n",
      "Epoch: 12/15... Step: 1620... Loss: 1.3539... Val Loss: 1.5067\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5806\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5617\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5241\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5237\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5282\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5165\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5105\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5043\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.4985\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.4989\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.4995\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5026\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5029\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5020\n",
      "Epoch: 12/15... Step: 1630... Loss: 1.3632... Val Loss: 1.5055\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5717\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5497\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5171\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5161\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5226\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5121\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5059\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5006\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.4939\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.4947\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.4965\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5010\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5011\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5006\n",
      "Epoch: 12/15... Step: 1640... Loss: 1.3561... Val Loss: 1.5040\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5671\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5480\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5119\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5083\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5164\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5072\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5018\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4965\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4914\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4921\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4933\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4968\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4975\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.4973\n",
      "Epoch: 12/15... Step: 1650... Loss: 1.3195... Val Loss: 1.5002\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.5712\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.5495\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.5096\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.5068\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.5151\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.5055\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4993\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4940\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4885\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4890\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4905\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4943\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4944\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4944\n",
      "Epoch: 12/15... Step: 1660... Loss: 1.3803... Val Loss: 1.4975\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5680\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5489\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5142\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5120\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5188\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5085\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.5018\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4953\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4887\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4898\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4907\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4950\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4951\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4947\n",
      "Epoch: 13/15... Step: 1670... Loss: 1.3593... Val Loss: 1.4984\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.5601\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.5454\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.5084\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.5070\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.5118\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.5025\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4965\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4906\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4843\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4859\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4876\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4914\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4918\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4907\n",
      "Epoch: 13/15... Step: 1680... Loss: 1.3568... Val Loss: 1.4946\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.5644\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.5462\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.5119\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.5096\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.5174\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.5061\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4994\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4925\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4873\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4885\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4895\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4929\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4920\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4918\n",
      "Epoch: 13/15... Step: 1690... Loss: 1.3497... Val Loss: 1.4954\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.5583\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.5373\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.5031\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.5019\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.5094\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4994\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4935\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4869\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4815\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4827\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4849\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4887\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4889\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4882\n",
      "Epoch: 13/15... Step: 1700... Loss: 1.3443... Val Loss: 1.4914\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.5673\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.5465\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.5106\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.5088\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.5131\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.5022\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4966\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4905\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4851\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4861\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4878\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4914\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4917\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4912\n",
      "Epoch: 13/15... Step: 1710... Loss: 1.3139... Val Loss: 1.4949\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.5684\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.5432\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.5087\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.5054\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.5121\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.5016\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4949\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4900\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4837\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4849\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4863\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4895\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4894\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4880\n",
      "Epoch: 13/15... Step: 1720... Loss: 1.3332... Val Loss: 1.4919\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.5641\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.5421\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.5081\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.5030\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.5109\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.5012\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4954\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4887\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4840\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4851\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4859\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4898\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4896\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4891\n",
      "Epoch: 13/15... Step: 1730... Loss: 1.3615... Val Loss: 1.4925\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.5573\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.5399\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.5032\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.5022\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.5089\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4988\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4916\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4857\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4801\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4815\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4825\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4860\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4861\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4853\n",
      "Epoch: 13/15... Step: 1740... Loss: 1.3346... Val Loss: 1.4886\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.5567\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.5405\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.5053\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.5035\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.5091\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4978\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4921\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4865\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4807\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4811\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4824\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4861\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4868\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4860\n",
      "Epoch: 13/15... Step: 1750... Loss: 1.3027... Val Loss: 1.4893\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.5559\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.5343\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4979\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4968\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.5022\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4922\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4871\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4811\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4750\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4754\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4769\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4811\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4814\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4806\n",
      "Epoch: 13/15... Step: 1760... Loss: 1.3355... Val Loss: 1.4846\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.5590\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.5345\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4992\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4960\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.5016\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4916\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4858\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4794\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4741\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4743\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4756\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4799\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4804\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4793\n",
      "Epoch: 13/15... Step: 1770... Loss: 1.3418... Val Loss: 1.4825\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.5524\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.5298\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4945\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4945\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.5002\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4900\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4838\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4788\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4735\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4752\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4770\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4808\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4810\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4801\n",
      "Epoch: 13/15... Step: 1780... Loss: 1.3257... Val Loss: 1.4837\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.5522\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.5309\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4963\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4922\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4980\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4899\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4844\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4781\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4720\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4727\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4746\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4792\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4794\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4792\n",
      "Epoch: 13/15... Step: 1790... Loss: 1.3113... Val Loss: 1.4827\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.5546\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.5282\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4925\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4911\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4964\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4867\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4810\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4739\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4693\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4703\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4721\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4764\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4770\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4769\n",
      "Epoch: 13/15... Step: 1800... Loss: 1.3348... Val Loss: 1.4800\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.5634\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.5408\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.5060\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.5037\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.5086\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4993\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4933\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4871\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4808\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4817\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4832\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4871\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4873\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4872\n",
      "Epoch: 14/15... Step: 1810... Loss: 1.3467... Val Loss: 1.4912\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.5496\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.5269\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4933\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4900\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4966\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4853\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4787\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4726\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4669\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4674\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4687\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4729\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4734\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4729\n",
      "Epoch: 14/15... Step: 1820... Loss: 1.3235... Val Loss: 1.4768\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.5448\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.5221\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4867\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4852\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4932\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4834\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4774\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4712\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4663\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4669\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4679\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4722\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4723\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4720\n",
      "Epoch: 14/15... Step: 1830... Loss: 1.3371... Val Loss: 1.4761\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.5554\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.5316\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4935\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4894\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4958\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4864\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4813\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4748\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4678\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4687\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4697\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4743\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4742\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4733\n",
      "Epoch: 14/15... Step: 1840... Loss: 1.2798... Val Loss: 1.4770\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.5467\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.5209\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4842\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4838\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4910\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4805\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4741\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4680\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4628\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4644\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4653\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4695\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4703\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4706\n",
      "Epoch: 14/15... Step: 1850... Loss: 1.2695... Val Loss: 1.4739\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.5501\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.5229\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4878\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4829\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4879\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4795\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4735\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4658\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4611\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4616\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4633\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4671\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4679\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4678\n",
      "Epoch: 14/15... Step: 1860... Loss: 1.3341... Val Loss: 1.4712\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.5464\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.5216\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4883\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4843\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4906\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4795\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4749\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4676\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4624\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4635\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4645\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4685\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4695\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4687\n",
      "Epoch: 14/15... Step: 1870... Loss: 1.3354... Val Loss: 1.4721\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.5385\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.5191\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4837\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4812\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4873\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4777\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4728\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4671\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4618\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4622\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4634\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4681\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4685\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4681\n",
      "Epoch: 14/15... Step: 1880... Loss: 1.3242... Val Loss: 1.4706\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.5493\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.5202\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4849\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4826\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4889\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4789\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4722\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4663\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4608\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4616\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4631\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4677\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4683\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4677\n",
      "Epoch: 14/15... Step: 1890... Loss: 1.3449... Val Loss: 1.4712\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.5529\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.5231\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4852\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4810\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4868\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4773\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4720\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4661\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4614\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4615\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4621\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4665\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4664\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4655\n",
      "Epoch: 14/15... Step: 1900... Loss: 1.3168... Val Loss: 1.4689\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.5401\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.5194\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4839\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4808\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4873\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4783\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4717\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4646\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4579\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4580\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4597\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4640\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4641\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4636\n",
      "Epoch: 14/15... Step: 1910... Loss: 1.3209... Val Loss: 1.4673\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.5309\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.5091\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4734\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4727\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4796\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4698\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4628\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4575\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4520\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4533\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4550\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4591\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4594\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4590\n",
      "Epoch: 14/15... Step: 1920... Loss: 1.3113... Val Loss: 1.4621\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.5339\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.5060\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4704\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4670\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4749\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4679\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4623\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4562\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4506\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4519\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4530\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4575\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4585\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4573\n",
      "Epoch: 14/15... Step: 1930... Loss: 1.2801... Val Loss: 1.4604\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.5348\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.5083\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4702\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4699\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4792\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4703\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4653\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4596\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4545\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4551\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4570\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4610\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4610\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4605\n",
      "Epoch: 14/15... Step: 1940... Loss: 1.3272... Val Loss: 1.4642\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.5360\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.5153\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4799\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4802\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4862\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4761\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4699\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4634\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4577\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4580\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4593\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4644\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4644\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4644\n",
      "Epoch: 15/15... Step: 1950... Loss: 1.3084... Val Loss: 1.4682\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.5356\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.5107\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4758\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4731\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4780\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4682\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4606\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4542\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4482\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4487\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4495\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4533\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4538\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4533\n",
      "Epoch: 15/15... Step: 1960... Loss: 1.3156... Val Loss: 1.4566\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.5355\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.5087\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4725\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4676\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4728\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4634\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4570\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4511\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4455\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4467\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4473\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4512\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4514\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4514\n",
      "Epoch: 15/15... Step: 1970... Loss: 1.2911... Val Loss: 1.4542\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.5511\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.5169\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4789\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4737\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4791\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4683\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4626\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4565\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4501\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4508\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4529\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4569\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4573\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4566\n",
      "Epoch: 15/15... Step: 1980... Loss: 1.2885... Val Loss: 1.4600\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.5378\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.5120\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4738\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4710\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4768\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4680\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4611\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4539\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4480\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4488\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4501\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4538\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4540\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4534\n",
      "Epoch: 15/15... Step: 1990... Loss: 1.2809... Val Loss: 1.4568\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.5345\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.5084\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4701\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4686\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4718\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4610\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4560\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4492\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4433\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4429\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4449\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4488\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4490\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4481\n",
      "Epoch: 15/15... Step: 2000... Loss: 1.2752... Val Loss: 1.4520\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.5425\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.5095\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4722\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4688\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4736\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4632\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4565\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4499\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4444\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4452\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4458\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4504\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4500\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4496\n",
      "Epoch: 15/15... Step: 2010... Loss: 1.2898... Val Loss: 1.4526\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.5323\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.5068\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4717\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4699\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4757\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4631\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4562\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4494\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4434\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4436\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4447\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4489\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4488\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4482\n",
      "Epoch: 15/15... Step: 2020... Loss: 1.3147... Val Loss: 1.4520\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.5405\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.5103\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4717\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4659\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4723\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4618\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4540\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4475\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4424\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4433\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4446\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4491\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4495\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4489\n",
      "Epoch: 15/15... Step: 2030... Loss: 1.2800... Val Loss: 1.4522\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.5393\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.5045\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4677\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4640\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4705\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4608\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4537\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4475\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4423\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4426\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4437\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4475\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4474\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4469\n",
      "Epoch: 15/15... Step: 2040... Loss: 1.2959... Val Loss: 1.4504\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.5488\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.5106\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4727\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4674\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4714\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4604\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4540\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4470\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4410\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4413\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4425\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4459\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4465\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4454\n",
      "Epoch: 15/15... Step: 2050... Loss: 1.2837... Val Loss: 1.4490\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.5346\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.5061\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4644\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4628\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4691\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4580\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4510\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4452\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4397\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4400\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4421\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4462\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4464\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4460\n",
      "Epoch: 15/15... Step: 2060... Loss: 1.2821... Val Loss: 1.4499\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.5333\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.5026\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4637\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4608\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4653\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4558\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4494\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4433\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4371\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4371\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4382\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4423\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4423\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4425\n",
      "Epoch: 15/15... Step: 2070... Loss: 1.3030... Val Loss: 1.4457\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.5456\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.5134\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4707\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4679\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4739\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4630\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4565\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4500\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4438\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4434\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4450\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4495\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4496\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4488\n",
      "Epoch: 15/15... Step: 2080... Loss: 1.2919... Val Loss: 1.4519\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 15 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU5Y0SThVVdE"
   },
   "outputs": [],
   "source": [
    "save_path = 'drive/MyDrive/Colab Notebooks/net.pth'\n",
    "torch.save(net.state_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7l3M9-en3V1"
   },
   "source": [
    "### **Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YStn8rqBghlt"
   },
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_15_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J9d6Aq2odeM"
   },
   "source": [
    "### **Make Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcWvXvg7ghlt"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "  ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "  # tensor inputs\n",
    "  x = np.array([[net.char2int[char]]])\n",
    "  x = one_hot_encode(x, len(net.chars))\n",
    "  inputs = torch.from_numpy(x)\n",
    "        \n",
    "      \n",
    "        \n",
    "  # detach hidden state from history\n",
    "  h = tuple([each.data for each in h])\n",
    "  # get the output of the model\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the character probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "    \n",
    "        \n",
    "  # get top characters\n",
    "  if top_k is None:\n",
    "    top_ch = np.arange(len(net.chars))\n",
    "  else:\n",
    "    p, top_ch = p.topk(top_k)\n",
    "    top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "    # select the likely next character with some element of randomness\n",
    "  p = p.numpy().squeeze()\n",
    "  char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DnP_35IoojV"
   },
   "source": [
    "### **Prime and Generate Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwuTJgQcghlu"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "  net.eval() # eval mode\n",
    "    \n",
    "  # First off, run through the prime characters\n",
    "  chars = [ch for ch in prime]\n",
    "  h = net.init_hidden(1)\n",
    "  for ch in prime:\n",
    "    char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "  chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "  for ii in range(size):\n",
    "    char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "    chars.append(char)\n",
    "\n",
    "  return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1439,
     "status": "ok",
     "timestamp": 1650665022908,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "DmFpWWiqghlu",
    "outputId": "94d1c1fb-f71e-4236-88b0-cba4d7d38db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, and treating him. \"You've\n",
      "come thought as, the croase, and then you're taken im to see your\n",
      "proper of my, but I'm general abready.\"\n",
      "\n",
      "\"Why is, you've been more threakenses, and I've say any more, and have a give\n",
      "moment to be self.\"\n",
      "\n",
      "\"When you were not, it's all all that I does are so meant to the ceasing that\n",
      "their pity and the peasants of countes and meets?\"\n",
      "\n",
      "\"Oh, you meant! I could stay a man, we danger as he was so a man of men. It's a\n",
      "second mother with myself on the peasants. I don't say that,\" she added,\n",
      "with his sense in the peasant, he saw what she was taking in the consideration. As they said to her\n",
      "than and to be a good and attaired to the table. He was nevired the same\n",
      "thing, his from the marsh had talked to be sering a motern of some\n",
      "carming on to her. They were stonition, and with a freenor\n",
      "on the seminite position to be sud all steps on the starter of seeming.\n",
      "\n",
      "\"I won't the men, I went to stop him, I'm\n",
      "at once to see the capital of my child are trustful about the candites\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t62mDI-Co1BY"
   },
   "source": [
    "### **Load Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1650665446638,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "ixUBYqmxghlv",
    "outputId": "616868bd-2f01-4a52-87d3-0a4dd99a8d8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_15_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1650665037865,
     "user": {
      "displayName": "Nicholas Edoseghe",
      "userId": "00121592726822803836"
     },
     "user_tz": -60
    },
    "id": "N3SF6A2Rghlv",
    "outputId": "f404491b-f8d5-4ddd-9358-4eab043f3d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said\n",
      "he had sat difficult see in at it.\n",
      "\n",
      "\"I walk a great means to take her. Where are you thinks. I should be so as all\n",
      "there all was to see this that.\"\n",
      "\n",
      "Alexey Alexandrovitch seemed, and heart toos he would be thanding horrily and things\n",
      "that he was answer and sounds. Seryozha. At the same things of the calmest and at\n",
      "an arm things and feeling of seesed his brother, and the mother that she\n",
      "was the baby time, and went out of sincere.\n",
      "\n",
      "\"I have no morning and so telling the significance of myself without\n",
      "my bing and means, the man, I've been dear in the foots and\n",
      "simply our feeling of them. Tis seciniment too. I'm taken to\n",
      "say it.\"\n",
      "\n",
      "\"Oh! It are so much and happy to the looking, and wanted to see alr marsh.\n",
      "This went in this care to him,\" he said to him.\n",
      "\n",
      "\"I don't know her father's easy,\" he added, said to his brother's\n",
      "admather, and that staying with the same tone of his\n",
      "wife, send on the stury, and shinking off his face and treeting\n",
      "some for the posetter of her assent face that so that he could not help\n",
      "a comment of the people still been said in a pity and a stor of the\n",
      "same than he was a contrish.\n",
      "\n",
      "\"Is it is there and a lengers of condisting of a man\n",
      "work to so at the same for success of\n",
      "the conversation, and so that it must be all so life to make the peecial father in some minure\n",
      "for me, with me and tried to be. What are the same men.\"\n",
      "\n",
      "\"You stor still mistrys. What are your sincers, but you want to be darted\n",
      "into her table! Alexey Alexandrovitch, all, will be a mile. Have you\n",
      "at her, things things all together.\"\n",
      "\n",
      "\"Yas must get it all, that is that she has been been,\n",
      "seanding him to that so mean in his husband. And to see ham, and then a sounds as\n",
      "a\n",
      "mother, the cancome time. If you could not talk to the most man thoughts anything; I\n",
      "have been set it.\"\n",
      "\n",
      "He saw that, would not hold it, how happy. But that were a\n",
      "standing, and the terribe of his carriages and him of intellections were to hear\n",
      "himself, and that will, a change that he frown at him in the soft of tone that\n",
      "sh\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
